{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-required-libraries-and-set-configurations\" data-toc-modified-id=\"Import-required-libraries-and-set-configurations-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import required libraries and set configurations</a></span></li><li><span><a href=\"#Import-required-dataset\" data-toc-modified-id=\"Import-required-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import required dataset</a></span></li><li><span><a href=\"#Train/Test-split\" data-toc-modified-id=\"Train/Test-split-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train/Test split</a></span></li><li><span><a href=\"#Specify-X-and-y-for-train-and-test\" data-toc-modified-id=\"Specify-X-and-y-for-train-and-test-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Specify X and y for train and test</a></span></li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Selection</a></span></li><li><span><a href=\"#Scale-the-appropriate-columns\" data-toc-modified-id=\"Scale-the-appropriate-columns-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Scale the appropriate columns</a></span></li><li><span><a href=\"#Model-Build\" data-toc-modified-id=\"Model-Build-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model Build</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from dask.distributed import Client\n",
    "from dask_searchcv import GridSearchCV\n",
    "from dask.diagnostics import ProgressBar\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration variables\n",
    "DATA_OUTPUT = '../data/processed/'\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataframe is :  (6798, 183)\n"
     ]
    }
   ],
   "source": [
    "# Read the pickle file\n",
    "df = pd.read_pickle(os.path.join(DATA_OUTPUT, \"master.pickle\"))\n",
    "\n",
    "# Check the shape of the file for consistency\n",
    "print(\"Shape of the dataframe is : \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "\n",
    "The time frame of the data is from Apr-Sept. There are only few records of Apr (around 3-4). We shall be excluding them. The train data would consist of data from Months May-Aug, while test data would be Sep data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data is :  (6601, 183)\n",
      "Shape of the testing data is :  (194, 183)\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test\n",
    "train = df[df.index.str.contains(\"May|June|July|August\")]\n",
    "test = df[df.index.str.contains(\"September\")]\n",
    "\n",
    "# Print the dimensions of the train and test\n",
    "print(\"Shape of the training data is : \", train.shape)\n",
    "print(\"Shape of the testing data is : \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify X and y for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify target\n",
    "target_cols = \"Quantity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train\n",
    "X_train = train.copy().drop(target_cols, axis=1)\n",
    "y_train = train[target_cols].tolist()\n",
    "\n",
    "# For test\n",
    "X_test = test.copy().drop(target_cols, axis=1)\n",
    "y_test = test[target_cols].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "We shall be selecting features with variance threshold of at least 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the threshold\n",
    "fs = VarianceThreshold(threshold=.05)\n",
    "\n",
    "# Select features \n",
    "fs.fit(X_train)\n",
    "X_train = X_train[X_train.columns[fs.get_support(indices=True)]]\n",
    "X_test = X_test[X_test.columns[fs.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the appropriate columns\n",
    "\n",
    "We shall be scalling **NominalofPulsa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean and standard deviation of NominalofPulsa of the train and scale wrt to that\n",
    "mean = X_train['NominalofPulsa'].mean()\n",
    "std = X_train['NominalofPulsa'].std()\n",
    "X_train['NominalofPulsa'] = X_train['NominalofPulsa'].apply(lambda x : ((x-mean)/std))\n",
    "X_test['NominalofPulsa'] = X_test['NominalofPulsa'].apply(lambda x : ((x-mean)/std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Build\n",
    "\n",
    "We shall be doing hyper parameter tuning with cross validation Also to optimize for efficiency, we shall be utilizing dask for grid search as it removes some amount of overhead which cannot be avoided with sklearns version of grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 37.9s\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters to tune on\n",
    "params = dict(n_estimators=[50, 100, 200], \n",
    "              max_depth=[3, 6, 10],\n",
    "              gamma=[0.1, 0.2, 0.3])\n",
    "\n",
    "# Intialize the model\n",
    "model = XGBRegressor(random_state=1, \n",
    "                     n_jobs=-1, \n",
    "                     subsample = 0.70, \n",
    "                     colsample_bytree = 0.7, \n",
    "                     eta = 0.05)\n",
    "\n",
    "# Initize the grid\n",
    "grid = GridSearchCV(model, \n",
    "                    params, \n",
    "                    n_jobs=-1,\n",
    "                    cv=5)\n",
    "\n",
    "# Fit the grid\n",
    "with ProgressBar():\n",
    "    grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model parameters\n",
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training predictions\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "# Testing predictions\n",
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE score for Train data is :  29.570597000075878\n",
      "MAPE score for Test data is :  378.4300524671593\n"
     ]
    }
   ],
   "source": [
    "# MAPE Score\n",
    "print('MAPE score for Train data is : ',\n",
    "      mean_absolute_percentage_error(y_train, train_pred))\n",
    "print('MAPE score for Test data is : ',\n",
    "      mean_absolute_percentage_error(y_test, test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
