{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dependencies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#System-architecture\" data-toc-modified-id=\"System-architecture-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>System architecture</a></span></li></ul></li><li><span><a href=\"#Observations\" data-toc-modified-id=\"Observations-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Observations</a></span></li><li><span><a href=\"#Data-FAQs:\" data-toc-modified-id=\"Data-FAQs:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data FAQs:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-has-this-been-approached-as-a-regular-regression-problem-and-not-a-time-series-problem?\" data-toc-modified-id=\"Why-has-this-been-approached-as-a-regular-regression-problem-and-not-a-time-series-problem?-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Why has this been approached as a regular regression problem and not a time series problem?</a></span></li><li><span><a href=\"#At-what-level-has-the-data-been-aggregated-at-and-why?\" data-toc-modified-id=\"At-what-level-has-the-data-been-aggregated-at-and-why?-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>At what level has the data been aggregated at and why?</a></span></li><li><span><a href=\"#What-is-lag-variable-and-why-has-it-been-added?\" data-toc-modified-id=\"What-is-lag-variable-and-why-has-it-been-added?-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>What is lag variable and why has it been added?</a></span></li><li><span><a href=\"#Forecast-sales-or-forecast-quantities?\" data-toc-modified-id=\"Forecast-sales-or-forecast-quantities?-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Forecast sales or forecast quantities?</a></span></li><li><span><a href=\"#Why-were-Quantity-and-Sellingprice-not-used-for-modelling?\" data-toc-modified-id=\"Why-were-Quantity-and-Sellingprice-not-used-for-modelling?-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Why were Quantity and Sellingprice not used for modelling?</a></span></li><li><span><a href=\"#Why-have-the-dummy-variables-been-made-on-n-1-categories?\" data-toc-modified-id=\"Why-have-the-dummy-variables-been-made-on-n-1-categories?-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Why have the dummy variables been made on n-1 categories?</a></span></li><li><span><a href=\"#Why-has-the-target-variable-been-capped?\" data-toc-modified-id=\"Why-has-the-target-variable-been-capped?-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Why has the target variable been capped?</a></span></li></ul></li><li><span><a href=\"#Modelling-FAQs:\" data-toc-modified-id=\"Modelling-FAQs:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modelling FAQs:</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-are-the-models-which-were-tried-and-why?\" data-toc-modified-id=\"What-are-the-models-which-were-tried-and-why?-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>What are the models which were tried and why?</a></span></li><li><span><a href=\"#What-were-the-metrics-which-were-looked-at-and-why?\" data-toc-modified-id=\"What-were-the-metrics-which-were-looked-at-and-why?-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>What were the metrics which were looked at and why?</a></span></li><li><span><a href=\"#Why-was-the-train/test-split-not-performed-randomly?\" data-toc-modified-id=\"Why-was-the-train/test-split-not-performed-randomly?-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Why was the train/test split not performed randomly?</a></span></li><li><span><a href=\"#Why-have-features-been-selected-at-10%-variance-threshold?\" data-toc-modified-id=\"Why-have-features-been-selected-at-10%-variance-threshold?-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Why have features been selected at 10% variance threshold?</a></span></li><li><span><a href=\"#Why-scale-only-NominalofPulsa-and-not-Lag1?\" data-toc-modified-id=\"Why-scale-only-NominalofPulsa-and-not-Lag1?-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Why scale only NominalofPulsa and not Lag1?</a></span></li><li><span><a href=\"#Why-is-the-objective-set-as-reg:tweedie-for-boosting-tree-models?\" data-toc-modified-id=\"Why-is-the-objective-set-as-reg:tweedie-for-boosting-tree-models?-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Why is the objective set as reg:tweedie for boosting tree models?</a></span></li><li><span><a href=\"#What-was-done-to-ensure-the-best-practices-for-modelling-were-followed?\" data-toc-modified-id=\"What-was-done-to-ensure-the-best-practices-for-modelling-were-followed?-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>What was done to ensure the best practices for modelling were followed?</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.6.2 64bit [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]"
        },
        {
         "module": "IPython",
         "version": "6.2.1"
        },
        {
         "module": "OS",
         "version": "Darwin 18.2.0 x86_64 i386 64bit"
        },
        {
         "module": "pandas",
         "version": "0.23.4"
        },
        {
         "module": "numpy",
         "version": "1.14.2"
        },
        {
         "module": "swifter",
         "version": "0.225"
        },
        {
         "module": "dask",
         "version": "0.19.1"
        },
        {
         "module": "dask_searchcv",
         "version": "0.2.0"
        },
        {
         "module": "sklearn",
         "version": "0.20.0"
        },
        {
         "module": "xgboost",
         "version": "0.80"
        },
        {
         "module": "lightgbm",
         "version": "2.2.2"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.2 64bit [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]</td></tr><tr><td>IPython</td><td>6.2.1</td></tr><tr><td>OS</td><td>Darwin 18.2.0 x86_64 i386 64bit</td></tr><tr><td>pandas</td><td>0.23.4</td></tr><tr><td>numpy</td><td>1.14.2</td></tr><tr><td>swifter</td><td>0.225</td></tr><tr><td>dask</td><td>0.19.1</td></tr><tr><td>dask_searchcv</td><td>0.2.0</td></tr><tr><td>sklearn</td><td>0.20.0</td></tr><tr><td>xgboost</td><td>0.80</td></tr><tr><td>lightgbm</td><td>2.2.2</td></tr><tr><td colspan='2'>Sun Nov 18 22:03:45 2018 IST</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.6.2 64bit [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] \\\\ \\hline\n",
       "IPython & 6.2.1 \\\\ \\hline\n",
       "OS & Darwin 18.2.0 x86\\_64 i386 64bit \\\\ \\hline\n",
       "pandas & 0.23.4 \\\\ \\hline\n",
       "numpy & 1.14.2 \\\\ \\hline\n",
       "swifter & 0.225 \\\\ \\hline\n",
       "dask & 0.19.1 \\\\ \\hline\n",
       "dask_searchcv & 0.2.0 \\\\ \\hline\n",
       "sklearn & 0.20.0 \\\\ \\hline\n",
       "xgboost & 0.80 \\\\ \\hline\n",
       "lightgbm & 2.2.2 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Sun Nov 18 22:03:45 2018 IST} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.6.2 64bit [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n",
       "IPython 6.2.1\n",
       "OS Darwin 18.2.0 x86_64 i386 64bit\n",
       "pandas 0.23.4\n",
       "numpy 1.14.2\n",
       "swifter 0.225\n",
       "dask 0.19.1\n",
       "dask_searchcv 0.2.0\n",
       "sklearn 0.20.0\n",
       "xgboost 0.80\n",
       "lightgbm 2.2.2\n",
       "Sun Nov 18 22:03:45 2018 IST"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%version_information pandas, numpy, swifter, dask, dask_searchcv, sklearn, xgboost, lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebooks have been compiled on 2015 version of Macbook. It has an i5 processor with 16 GB RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Data is collected at the daily level (From Apr-Sept).\n",
    "+ Data contains information of different mobile operators.\n",
    "+ There are 1983 outlets.\n",
    "+ There are 6 operators and 10 brands.\n",
    "+ There are 56 cities.\n",
    "+ There are 11 sub regions.\n",
    "+ There are 4 outlet sizes.\n",
    "+ There are 5 outlet brandings.\n",
    "+ There are 7 type of packs and 3 types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data FAQs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why has this been approached as a regular regression problem and not a time series problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sole reason behind my decision to approach this as a regular regression problem is because:\n",
    "+ For this problem statement, we need to make a time series model for each **Operator** and each **id_outlet** following the logic that different operators and different outlets may have different patterns.\n",
    "+ Time series work best if one has at least 2 years of data in my experience. For our problem statement we have data only from April to September.\n",
    "+ Even though the data is at a daily level, there are many missing dates. We would need to generate this dates and impute total sales for them. This could skew the results of the model.\n",
    "+ The dataset contains rich variables like **CITY**, **SUB REGION**, **Outlet Size** etc which could be best exploited by machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At what level has the data been aggregated at and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been aggregated at **id_outlet** and **Month** (extracted from Date) level. The intuition behind this is that since time series models was not on the table for us, the only way we could bring the behaviour of each **id_outlet** with respect to time was making a key concatenating the **id_outet** and **Month** and aggregating on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is lag variable and why has it been added?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem statement is to :\n",
    "\n",
    "+ **Forecast for next month**\n",
    "+ **Forecast for the next three months**\n",
    "\n",
    "Since we have translated our problem statement from a time-series to a regression problem, the only way we can answer the above two questions is by adding a lag variable. \n",
    "\n",
    "For the first question we have added a **Lag1** variable by shifting **TotalSellingPrice** by 30. What this means is that **given a price today, what is the expected price 30 days into the future**.\n",
    "\n",
    "For the second question all we need to do is go to the **01_Data Preparation.ipynb** and change the lag by 90 and re-run both **01_Data Preparation.ipynb** and **02_Model Build.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecast sales or forecast quantities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a cursory exploratory data analysis of Quantities, it was found that **99% of the data has Quantity = 1**. After aggregation at **id_outlet** and **Month** level, both the **Quantity** and lagged version of **Quantity** were the same for this reason and could result into a false accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why were Quantity and Sellingprice not used for modelling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TotalSellingPrice** has a direct relationship with both the quantities above i.e. **TotalSellingPrice = Quantity x Sellingprice**. \n",
    "\n",
    "Also going by future deployment aspects, since the data has been aggreagated at a monthly level (and id_outlet level) the question which needs to be answered is:\n",
    "\n",
    "**For a record coming in at day 0, are monthly quantities and selling price known?** \n",
    "\n",
    "The answer to the above question is a no and hence these have not been used for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why have the dummy variables been made on n-1 categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classic dummy trap. Categorical information can always be represented by n-1 levels. Case in point, for a variable like **Gender** we only need one dummy variable denoting male or female i.e. either **is_male OR is_female**. We do not need both. **Representing as n categories will lead to multicollinearity** unless the effect has been muted by **regularization**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why has the target variable been capped?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have capped the target variable at the 99th percentile value as having few large values could skew our model predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling FAQs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the models which were tried and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models tried were **Ridge Regression, Support Vector Machines, Extreme Gradient Boosting, Light Gradient Boosting, Random Forest**. The logic behind this is as follows:\n",
    "\n",
    "+ **Ridge :** Linear regularization based non-tree model\n",
    "+ **SVM :** Non-linear kernel based non-tree model\n",
    "+ **XGBoost :** Non-linear ensemble boosting tree based model\n",
    "+ **LightGBM :** Non-linear ensemble boosting tree based model with option for boosting categorical features\n",
    "+ **RF :** Non-linear ensemble bagging tree based model\n",
    "\n",
    "\n",
    "The final model is **Stacking** of the models above which gave stable results (i.e. RMSE of train and test being very close by)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What were the metrics which were looked at and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked at **RMSE** and **MAPE** for three reasons:\n",
    "+ **MAPE** is a popular metric to evaluate time series model.\n",
    "+ **RMSE** is a popular metric to evaluate a machine learning regression model.\n",
    "+ Since we have translated a time series problem to a regression problem, it makes sense to look at both metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why was the train/test split not performed randomly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not done a random train/test split for the simple reason that since we are representing time domain in our key itself, doing a random sampling will result in information leakage i.e. the model learns features it ideally should not because it should not be available to it. Hence we have done an out-of-time sampling approach keeping September data as our test set and everything before that as our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why have features been selected at 10% variance threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features are incredibly sparse and keeping them in the model only adds more noise to predictions as well as increase the computation time. Hence we have dropped variables according to the 10% variance threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why scale only NominalofPulsa and not Lag1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended for any machine learning process i.e. regression or classification to normalize/standardize numerical features. However, for the **Lag1** variable this is an exception, as we need those values as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is the objective set as reg:tweedie for boosting tree models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though Tweedie distributions are used to model loss almost exclusively in the Insurance industry, it was found to work well in our scenario as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was done to ensure the best practices for modelling were followed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Columns with high percentage of missing values have been dropped. The ones with low percentage of missing values were imputed with the median.\n",
    "+ Feature selection to filter out sparse columns to reduce noise.\n",
    "+ Parameter tuning to arrive at the best possible version of each model.\n",
    "+ Parameters tuned with custom scoring function as mape.\n",
    "+ Parameters tuned with 5 fold cross validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
